# Crítica : Evaluating Recommendation Systems
En este artículo Guy Shani y Asela Gunawardana discuten principalmente como comparar distintos tipos de experimentos en sistemas Recomendadores y muestran un resumen de las propiedades de cada tipo de experimento, como también formas de evaluar estos tipos de experimentos dependiendo de ciertas propiedades del mismo. Para esto, en primera instancia describen tres tipos de experimentos y sus propiedades: *offline, basados en el estudio de usuarios* y  *online*. Finalmente,  los autores discuten las propiedades de sistemas recomendadores que impactan en el desempeño de algoritmos y como medir estas propiedades, así como también, como los sistemas recomendadores pueden ser evaluados en orden para seleccionar el mejor algoritmo dentro de un conjunto de candidatos.

En primera instancia me gustaría destacar que es un artículo que es muy fácil de comprender y de seguirle el ritmo. Me parece un excelente *approach*  para todo lo que hemos visto en el curso y posee un muy buen resumen acerca de las métricas de desempeño de ciertas propiedades en los sistemas recomendadores (que inclusive no hemos visto).

Siguiendo la misma linea, me parece que toda la sección de experimentos con datos *offline* será de mucha ayuda de cara a la tarea del curso que nos viene por delante. Especialmente en la parte en donde se especifica la reducción de sesgos de la información y como se puede mejorar la utilización de los datos para tener una muestra más fidedigna.

No obstante, me parece que se comete un  error al no considerar la validación estadística como una directriz dentro de los estudios de los experimentos. Pese a que en el desarrollo del texto se realizan ciertas pruebas estadísticas en los experimentos ( comparación de valores p, tablas ANOVA, estudio de intervalos de confianza, tests de bondad de ajuste, etc.) durante todo el desarrollo se tratan como formas externas de evaluación del poder de generalización de los experimentos, sin embargo,  a mi parecer,  son una parte critica del desarrollo de cualquier experimento (por ejemplo, para comprender el comportamiento de los datos) y con ello,  el criterio de validación estadística se debiese haber incluido dentro de los pilares fundamentales dentro de la experimentación ( no solo como algo que otorga poder de generalización a un cierto modelo ).

Para concluir, me gustaría recalcar que el articulo es una muy buena forma de acercarnos a la comparación de sistemas Recomendadores, algo que es fundamental al momento de buscar algoritmos que se ajusten a las necesidades de cada problema y también al momento de pensar en formas de mejorar los mismos. 
